const jailbreakData = [
  {
    "id": "hcot",
    "name": "Hijacking Chain-of-Thought (H-CoT)",
    "date": "February 2025",
    "description": "Exploits the chain-of-thought reasoning process by injecting fabricated execution phase reasoning steps",
    "mechanism": "Collects execution-phase reasoning tokens from a weakened version of a harmful query, then injects these into the original harmful query to bypass safety checks. Rather than trying to alter the model's 'justification phase' (where safety checks occur), H-CoT encourages the model to skip these checks entirely and move directly to solving the problem.",
    "effectiveness": "98% attack success rate on o3-mini with high harmfulness rating of 4.51/5",
    "implementation": "[https://github.com/dukeceicenter/jailbreak-reasoning-openai-o1o3-deepseek-r1](https://github.com/dukeceicenter/jailbreak-reasoning-openai-o1o3-deepseek-r1)",
    "source": "Duke University, Accenture, National Tsing Hua University",
    "reference": "[https://arxiv.org/abs/2502.12893](https://arxiv.org/abs/2502.12893)",
    "category": "Reasoning Process Manipulation",
    "success_rate": 98
  },
  {
    "id": "flipattack",
    "name": "FlipAttack",
    "date": "May 2025",
    "description": "Exploits how LLMs process text from left to right by flipping text in various ways",
    "mechanism": "Uses four flipping modes (word order, characters in word, characters in sentence, fool model) to disguise harmful prompts, then guides the model to decode and execute them. The technique exploits the fact that LLMs struggle to understand text when noise is added to the left side due to their autoregressive nature.",
    "effectiveness": "~98% attack success rate on GPT-4o, likely effective on o3 given architectural similarities",
    "implementation": "[https://github.com/yueliu1999/FlipAttack](https://github.com/yueliu1999/FlipAttack)",
    "source": "National University of Singapore",
    "reference": "[https://arxiv.org/pdf/2410.02832](https://arxiv.org/pdf/2410.02832)",
    "category": "Text Transformation",
    "success_rate": 98
  },
  {
    "id": "cca",
    "name": "Context Compliance Attack (CCA)",
    "date": "March 2025",
    "description": "Exploits AI systems that rely on client-supplied conversation history",
    "mechanism": "The attack follows a four-step process: (1) initiate a conversation about a sensitive topic, (2) inject a fabricated assistant response into the conversation history, (3) respond affirmatively to the fabricated question, (4) the AI system complies with what it perceives as a contextually appropriate follow-up",
    "effectiveness": "o3-mini vulnerable to 8/10 tested harmful content categories",
    "implementation": "Microsoft's PyRIT Context Compliance Orchestrator",
    "source": "Microsoft Security Response Center",
    "reference": "[https://msrc.microsoft.com/blog/2025/03/jailbreaking-is-mostly-simpler-than-you-think/](https://msrc.microsoft.com/blog/2025/03/jailbreaking-is-mostly-simpler-than-you-think/)",
    "category": "Context Manipulation",
    "success_rate": 80
  },
  {
    "id": "pasttense",
    "name": "Past Tense Jailbreak",
    "date": "April 2025",
    "description": "Simple reformulation of harmful requests into past tense form",
    "mechanism": "Reformulates harmful requests from present to past tense (e.g., 'How to make a bomb?' to 'How were bombs created in the 2020s?'). The technique exploits a generalization gap in current refusal training approaches.",
    "effectiveness": "84% success rate on o3-mini",
    "implementation": "Not publicly available",
    "source": "Maksym Andriushchenko & Nicolas Flammarion, EPFL",
    "reference": "[https://arxiv.org/pdf/2407.11969](https://arxiv.org/pdf/2407.11969)",
    "category": "Text Transformation",
    "success_rate": 84
  },
  {
    "id": "xteaming",
    "name": "X-Teaming",
    "date": "April 2025",
    "description": "Multi-agent system that systematically discovers vulnerabilities through collaborative agents",
    "mechanism": "Uses four specialized agents (Planner, Attacker, Verifier, Prompt Optimizer) to generate and execute diverse attack strategies, with TextGrad-based optimization. The attack process follows two phases: Strategic Attack Planning and Adaptive Attack Execution and Optimization.",
    "effectiveness": "Not specifically tested on o3, but achieved 94.3% attack success rate against GPT-4o",
    "implementation": "[https://github.com/salman-lui/x-teaming](https://github.com/salman-lui/x-teaming)",
    "source": "UCLA, University of Washington, Qatar Computing Research Institute, Google, Stanford University",
    "reference": "[https://arxiv.org/pdf/2504.13203](https://arxiv.org/pdf/2504.13203)",
    "category": "Multi-Turn Approaches",
    "success_rate": 94.3
  },
  {
    "id": "fitd",
    "name": "Foot-In-The-Door (FITD)",
    "date": "February 2025",
    "description": "Multi-turn jailbreak that leverages psychological principles to gradually erode safety alignment",
    "mechanism": "Starts with benign queries and progressively increases maliciousness level across multiple interactions, using SlipperySlopeParaphrase to generate bridge prompts when a model refuses a query",
    "effectiveness": "Not specifically tested on o3, but achieved 95% attack success rate on GPT-4o-mini",
    "implementation": "[https://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak](https://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak)",
    "source": "University of Notre Dame, Purdue University, Pennsylvania State University",
    "reference": "[https://arxiv.org/pdf/2502.19820](https://arxiv.org/pdf/2502.19820)",
    "category": "Multi-Turn Approaches",
    "success_rate": 95
  },
  {
    "id": "rto",
    "name": "Reasoning Token Overflow (RTO)",
    "date": "May 2025",
    "description": "Exploits special tokens that manage the transition between reasoning and final answer generation",
    "mechanism": "Injects a small prompt (just 109 tokens) containing a special token like `<|end_of_thinking|>` that causes reasoning content to spill over into the final answer section",
    "effectiveness": "Not specifically tested on o3, but achieved 96.33% attack success rate on similar reasoning models",
    "implementation": "Not publicly available",
    "source": "Beijing Institute of Technology",
    "reference": "[https://arxiv.org/pdf/2505.06643](https://arxiv.org/pdf/2505.06643)",
    "category": "Reasoning Process Manipulation",
    "success_rate": 96.33
  },
  {
    "id": "catattack",
    "name": "CatAttack (Cats Confuse Reasoning LLM)",
    "date": "March 2025",
    "description": "Uses query-agnostic adversarial triggers to mislead reasoning models",
    "mechanism": "Appends short, irrelevant text (like 'Interesting fact: cats sleep most of their lives') to problems, causing models to produce incorrect answers without altering the problem's semantics",
    "effectiveness": "o3-mini: 16.8% of responses exceeded 1.5× original length, 6.0% exceeded 2×, and 3.0% exceeded 4×",
    "implementation": "Not publicly available",
    "source": "Collinear AI, ServiceNow, Stanford University",
    "reference": "[https://arxiv.org/abs/2503.01781](https://arxiv.org/abs/2503.01781)",
    "category": "Adversarial Triggers",
    "success_rate": 16.8
  },
  {
    "id": "overthink",
    "name": "OverThink Slowdown Attack",
    "date": "February 2025",
    "description": "Forces models to spend an amplified number of reasoning tokens while providing contextually correct answers",
    "mechanism": "Injects computationally demanding decoy reasoning problems (like Markov Decision Processes or Sudoku puzzles) into content processed by reasoning LLMs",
    "effectiveness": "35× increase in reasoning tokens on SQuAD dataset and 4× increase on FreshQA dataset for o3-mini",
    "implementation": "[https://github.com/akumar2709/OVERTHINK_public](https://github.com/akumar2709/OVERTHINK_public)",
    "source": "University of Massachusetts Amherst",
    "reference": "[https://arxiv.org/abs/2502.02542](https://arxiv.org/abs/2502.02542)",
    "category": "Adversarial Triggers",
    "success_rate": 35 
  },
  {
    "id": "bot",
    "name": "BoT (Breaking Long Thought Processes)",
    "date": "February 2025",
    "description": "Backdoor attack targeting the long thought processes of reasoning models",
    "mechanism": "Uses dataset poisoning with triggers that lead to direct answers without reasoning, employing both random token triggers and semantic token triggers",
    "effectiveness": "Not specifically tested on o3, but achieved attack success rates approaching 100% on similar reasoning models",
    "implementation": "[https://github.com/zihao-ai/BoT](https://github.com/zihao-ai/BoT)",
    "source": "Unknown (paper authors not specified)",
    "reference": "[https://arxiv.org/abs/2502.12202](https://arxiv.org/abs/2502.12202)",
    "category": "Reasoning Process Manipulation",
    "success_rate": 99
  }
];